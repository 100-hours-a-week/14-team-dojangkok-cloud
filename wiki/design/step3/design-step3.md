# 3단계 : CD 파이프라인 구축

- 작성일: 2026-01-08
- 최종수정일: 2026-01-26
## 개요

**도장콕**은 임차인의 부동산 임대 계약 과정(계약 전·계약 중)을 지원하는 서비스입니다. <br>
핵심 기능: 쉬운 계약서(해설+리스크 검증) | 집노트(답사/비교/체크리스트) | 임대 매물 커뮤니티

**본 문서**: CD(지속적 배포) 파이프라인 구축을 다룹니다. CI에서 생성된 아티팩트를 활용해 수동 트리거 기반의 In-place 배포를 구성하고, 롤백 절차를 정의합니다.

본 문서 범위: MVP 3단계 CD(지속적 배포) 파이프라인 설계·구축이며, 컨테이너화는 후속 단계에서 다룹니다.


## 목차

1. [도장콕 현황](#1-도장콕-현황)
   - [배포 대상 환경](#11-배포-대상-환경)
   - [현재 배포 프로세스](#12-현재-배포-프로세스)
2. [현재 상황의 문제점과 CD 필요성](#2-현재-상황의-문제점과-cd-필요성)
   - [위험 요소 분석](#21-위험-요소-분석)
   - [CD 도입 필요성](#22-cd-도입-필요성)
3. [CD 전략 선택](#3-cd-전략-선택)
   - [배포 트리거 방식](#31-배포-트리거-방식)
   - [배포 전략](#32-배포-전략)
   - [EC2 접근 방식](#33-ec2-접근-방식--ssh)
4. [배포 대상 및 트리거](#4-배포-대상-및-트리거)
5. [CD 파이프라인 단계](#5-cd-파이프라인-단계)
   - [공통 단계](#51-공통-단계)
   - [CI 아티팩트 활용](#52-ci-아티팩트-활용)
   - [서버별 특이사항 요약](#53-서버별-특이사항-요약)
6. [실패 시 대응 방안](#6-실패-시-대응-방안)
   - [롤백 절차](#61-롤백-절차)
   - [재시도 방식](#62-재시도-방식)
7. [다음 단계](#7-다음-단계)

---

## 1. 도장콕 현황

### 1.1 배포 대상 환경

```
도장콕 서비스
├── Frontend (React)      → EC2 CPU Instance (Nginx)
├── Backend (Spring Boot) → EC2 CPU Instance (systemd)
└── AI Server (FastAPI)   → GCP VM(GPU) (systemd)
```

- **인프라**: AWS EC2 (컨테이너 미사용), GCP VM(GPU)
- **웹서버**: Nginx (Frontend static 파일 서빙, 리버스 프록시)
- **프로세스 관리**: systemd (Backend, AI Server)

### 1.2 현재 배포 프로세스 (CD 미도입 상태)

| 항목 | 현재 상태 |
|------|----------|
| 배포 트리거 | 담당자가 수동으로 SSH 접속 |
| 배포 절차 | git pull → 빌드 → 서비스 재시작 |
| 배포 이력 | 터미널 히스토리에 의존 |
| 롤백 방법 | git checkout 후 재빌드 |
| 알림 | 없음 (구두 공유) |

<br>

## 2. 현재 상황의 문제점과 CD 필요성

### 2.1 위험 요소 분석

| 위험 요소 | 설명 |
|----------|------|
| **인적 오류 개입** | 사람이 직접 터미널에서 명령어 입력 → 오타, 누락, 잘못된 경로 등 실수 확률 증가 |
| **배포 속도/빈도 저하** | 파일 이동, 명령어 입력을 수동으로 수행 → 서비스 피드백 루프 길어짐 |
| **일관성/추적 어려움** | 배포 이력을 수동 관리하지 않으면 누가, 언제, 무엇을 배포했는지 추적 불가 |
| **장애 복구 난이도 상승** | 장애 시 당황한 상태에서 이전 버전 찾기 → 재빌드 → 전송 → 심리적 압박 + 복구 지연 + 추가 실수 유발 |

### 2.2 CD 도입 필요성

위 문제들을 방치하면 배포 장애 시 복구 시간이 길어진다.

| 상황 | 수동 배포 | CD 도입 후 |
|------|----------|----------|
| 롤백 소요 시간 | 3분~무한 (담당자 가용성 의존) | ~1분 20초 |
| 배포 이력 확인 | 터미널 히스토리 | CD 도구 대시보드 |
| 휴먼 에러 | 오타, 누락 가능 | 검증된 스크립트 실행 |
| 동시 배포 충돌 | 발생 가능 | 단일 워크플로우로 방지 |

CD 도입으로 위 문제들을 해결할 수 있다:

1. **표준화**: 모든 배포가 동일한 절차로 실행
2. **추적성**: CD 도구에서 배포 이력(누가, 언제, 무엇을) 자동 기록
3. **안전성**: 검증된 스크립트 실행, 휴먼 에러 방지
4. **알림**: 배포 시작/완료/실패 즉시 공유
5. **빠른 롤백**: 이전 배포 단위로 신속한 복구

<br>

## 3. CD 전략 선택

### 3.1 배포 트리거 방식 : 수동 트리거 (workflow_dispatch)

```yaml
on:
  workflow_dispatch:
    inputs:
      confirm:
        description: '배포를 진행하시겠습니까? (yes 입력)'
        required: true
```

#### 배포 트리거 방식 비교

| 방식 | 트리거 조건 | 장점 | 단점 |
|------|------------|------|------|
| **Push 자동 배포** | main 브랜치 push 시 | 즉시 배포, 빠른 피드백 | 검증 없이 프로덕션 반영 위험 |
| **PR Merge 자동 배포** | PR 머지 시 | Push보다 리뷰 단계 포함 | 머지 = 배포로 유연성 낮음 |
| **Tag 기반 배포** | 버전 태그 생성 시 | 버전 관리 명확 | 태그 생성 추가 작업 필요 |
| **수동 트리거** | GitHub UI에서 버튼 클릭 | 배포 시점 완전 통제 | 매번 수동 액션 필요 |
| **스케줄 배포** | cron 표현식 (예: 매주 수요일 14시) | 예측 가능한 배포 | 긴급 배포 별도 처리 필요 |

#### 선택 이유: 왜 자동 배포가 아닌 수동 트리거인가?

| 상황 | 자동 배포 리스크 | 수동 트리거 이점 |
|------|----------------|-----------------|
| DB 마이그레이션/배치 실행 중 | 배포로 인한 충돌/장애 | 완료 후 안전하게 배포 |
| 트래픽 피크 시간대 | 서비스 불안정 가능 | 한산한 시간에 배포 |
| 장애 대응 중 | 상황 악화 가능 | 복구 완료 후 배포 |
| 야간/주말 | 모니터링/롤백 인력 부재 | 근무 시간에만 배포 |

1. **운영 리스크를 사람이 최종 제어**
- 단일 인스턴스 in-place 배포 구조에서 자동 배포는 "지금 배포해도 되는 상태인가?"를 판단하지 못함
- DB 마이그레이션 중, 트래픽 피크, 장애 대응 중에 자동 배포되면 위험

2. **배포 타이밍 통제**
- 야간/주말 자동 배포를 막고, 모니터링/롤백 가능한 근무 시간에만 배포
- 배포 전 팀원에게 Discord 공유 후 진행하는 워크플로우에 적합

<br>


#### 향후 계획: 환경별 배포 방식 분리

현재는 프로덕션 단일 환경이지만, 개발 서버 구축 시 환경별로 다른 배포 방식 적용 예정:

| 환경 | 배포 방식 | 목적 |
|------|----------|------|
| **개발 서버** | CI 통과 시 자동 배포 | 개발 중 빠른 피드백 |
| **프로덕션 서버** | 수동 트리거 유지 | 안정성 우선 |


프로덕션 자동 배포 전환은 아래 조건 충족 시 검토:

| 조건 | 현재 상태 | 전환 기준 |
|------|----------|----------|
| **테스트 커버리지** | 테스트 미작성 | 핵심 사용자 플로우가 E2E 테스트로 커버됨 |
| **헬스체크 정확도** | 검증 전 | 최근 10회 배포에서 헬스체크가 실제 장애를 정확히 감지 |
| **에러 모니터링** | 미구축 | 에러 트래킹 도구로 배포 전후 에러율 비교 가능 |



<br>

### 3.2 배포 전략 : In-place 배포

#### 배포 전략 비교

| 전략 | 설명 | 다운타임 | 복잡도 | 필요 인프라 |
|------|------|---------|-------|------------|
| **In-place** | 기존 서버에서 직접 교체 | **있음** (서비스 재시작 시간) | 낮음 | 단일 서버 |
| Rolling | 서버를 순차적으로 업데이트 | 없음* | 중간 | 다중 서버 + 로드밸런서 |
| Blue-Green | 새 환경 준비 후 트래픽 전환 | 없음* | 높음 | 동일 인프라 2세트 |
| Canary | 일부 트래픽만 새 버전으로 | 없음* | 높음 | 트래픽 라우팅 인프라 |

> *다운타임 "없음"의 조건: 다중 서버/환경이 있어 트래픽을 분산하거나 전환할 수 있을 때 가능. 단일 서버 환경에서는 적용 불가.

### 선택 이유

1. **단일 인스턴스 운영**
   - 현재 CPU 인스턴스 1대, AI 인스턴스 1대로 운영
   - Rolling, Blue-Green은 다중 서버가 전제 → 해당 없음
   - 단일 서버에서는 In-place가 유일한 현실적 선택
   - 무중단 배포는 추가 인프라(2대 이상, LB/라우팅) 없이는 적용 불가

2. **인프라 비용 및 복잡도**
   - Blue-Green: 동일 인프라 2세트 필요 → 비용 2배
   - Rolling: 로드밸런서 + 다중 서버 필요 → 관리 포인트 증가
   - MVP 단계에서 추가 인프라 투자는 비효율적

3. **다운타임 허용 근거**
   - 현재 예상 트래픽: DAU 50~100명, 피크 동시접속 15~30명
   - 3분 다운타임 시 영향 범위:
     - 평균 분당 접속자: ~0.07명 (DAU 100 ÷ 24시간 ÷ 60분)
     - 피크 시간 분당 접속자: ~0.5명 (동시접속 30명 기준)
     - 진행 중 작업 중단 영향: 피크 시간 동시접속자 ~30명
     - **3분 다운타임 시 영향받는 사용자: 최대 5~10명** (신규 유입 + 진행 중 작업 중단)
   - In-Place 배포 방식으로 인한 리스크는 팀 차원에서 관리:
     - 적합한 배포 시간 선정 (새벽/점심 등 저트래픽 시간대)
     - 배포 시나리오 사전 검토
   - 결론: 현재 트래픽 규모에서 In-place 배포로 인한 사용자 영향은 미미

### **무중단 배포 전환 시점**

**전환 검토 시나리오:**
| 시나리오 | 상황 | 영향 |
|----------|------|------|
| DAU 증가 | DAU 1,000명 도달 | 3분 다운타임 시 ~2명 → ~20명 영향 |
| B2B 계약 | 부동산 중개소 제휴 | 업무 시간 내 서비스 가용성 계약 조건 |
| 24시간 서비스 | 야간 사용자 증가 | 새벽 배포 시간대 확보 불가 |

**전환 기준:**
- 3분 다운타임 시 예상 영향 사용자가 10명 이상이 되는 시점 (DAU ~500명)
- 또는 서비스 가용성 SLA가 계약 조건으로 명시되는 시점

> 위 기준에 ±20% 근접 시 Blue-Green 배포 검토 시작 (인프라 준비 기간 고려)


<br>

### 3.3 EC2 접근 방식 : SSH

#### EC2 접근/배포 방식 비교

| 방식 | 동작 원리 | 장점 | 단점 | 적합한 상황 |
|------|----------|------|------|------------|
| **SSH** | SSH 키로 EC2에 직접 접속하여 명령 실행 | 설정 간단, 익숙한 방식 | 22 포트 노출, 키 관리 필요 | **MVP/초기 단계**, 단일 인스턴스 |
| **SSM Run Command** | AWS API로 EC2에 명령 전달 (SSH 불필요) | 포트 불필요, CloudTrail 로그 | IAM Role/Agent 설정 필요 | 프라이빗 서브넷, 보안 강화 필요 시 |
| **OIDC + SSM** | GitHub→AWS 임시 토큰 발급 후 SSM 사용 | 장기 키 저장 없음, 브랜치별 권한 | 설정 복잡도 높음 | 보안 감사 요구, 다중 계정 |
| **CodeDeploy** | S3에 아티팩트 업로드 → CodeDeploy Agent가 배포 | 다중 인스턴스, 롤백 내장, Auto Scaling 연동 | Agent 설치, S3/IAM 설정 필요 | 다중 인스턴스, Auto Scaling 환경 |
| **Self-Hosted Runner** | EC2에서 직접 GitHub Actions 실행 | VPC 내부 리소스 접근 용이 | Runner 관리 필요, 비용 | 프라이빗 리소스 접근 필요 시 |

#### 상세 비교: SSH vs SSM vs OIDC

| 항목 | SSH | SSM | OIDC + SSM |
|------|-----|-----|------------|
| **네트워크** | 22 포트 인바운드 필요 | 인바운드 포트 불필요 | EC2와 무관 (GitHub→AWS STS) |
| **인증 방식** | SSH 개인키 (Secrets 저장) | 장기 Access Key 또는 OIDC | 단기 토큰 (워크플로우마다 발급) |
| **감사/로그** | 쉘 히스토리 수준 | CloudTrail + SSM 로그 | CloudTrail (Role 가정 기록) |
| **보안 리스크** | 키 유출, 포트 노출 | IAM 기반 접근통제 | 장기 키 없음, 브랜치별 제어 |
| **설정 복잡도** | 낮음 | 중간 | 높음 |

#### 선택 이유: MVP 단계에서 SSH

1. **설정 시간 최소화**
   - SSH 키 생성 → GitHub Secrets 등록 → 즉시 사용 가능
   - SSM: IAM Role 생성 + SSM Agent 설치 + 정책 연결 필요
   - OIDC: AWS에 OIDC Provider 등록 + Trust Policy 작성 + 클레임 조건 설정 필요

2. **러닝 커브**
   - SSH는 팀원 모두 익숙한 방식
   - SSM/OIDC는 AWS IAM 이해 필요

3. **현재 보안 요구 수준**
   - 초기 MVP, 제한된 트래픽/데이터
   - 22 포트는 GitHub Actions IP 대역으로 제한 가능

#### 향후 전환 검토 시점

| 조건 | 현재 상태 | 전환 기준 |
|------|----------|----------|
| **민감 데이터 처리** | 개인정보 최소 | 주민번호/금융정보 등 민감정보 취급 시 |
| **보안 감사 요구** | 없음 | 외부 보안 감사/인증 요구 시 (ISMS 등) |
| **프라이빗 서브넷 전환** | 퍼블릭 서브넷 | EC2가 프라이빗 서브넷으로 이동 시 |
| **다중 계정 운영** | 단일 계정 | dev/staging/prod 계정 분리 시 |

> 위 조건 중 하나 이상 해당 시 OIDC + SSM 조합으로 전환 검토


<br>

## 4. 브랜치 정책 및 CD 트리거

### 4.1 브랜치 정책

```
main ──────────────────────────── (프로덕션 배포 대상)
  │
  └── dev ─────────────────────── (개발 통합, 배포 대상 아님)
        │
        ├── feat/xxx ──────────── (기능 개발)
        └── fix/xxx ───────────── (버그 수정)
```

- **main 브랜치만 배포 대상**
- dev → main PR 머지 후 수동으로 CD 트리거

### 4.2 CD 트리거

| 이벤트 | 브랜치 | 실행 작업 | 비고 |
|--------|--------|----------|------|
| **workflow_dispatch** | `main` | CD 전체 파이프라인 | 수동 실행, confirm 입력 필요 |

### 4.3 workflow_dispatch 설정 예시

```yaml
on:
  workflow_dispatch:
    inputs:
      confirm:
        description: '배포를 진행하시겠습니까? (yes 입력)'
        required: true
        type: string

jobs:
  deploy:
    if: github.event.inputs.confirm == 'yes'
    runs-on: ubuntu-latest
    steps:
      # 배포 단계들...
```

<br>

## 5. CD 파이프라인 단계

### 5.1 공통 단계

| 단계 | 목적 | 실패 시 영향 |
|------|------|-------------|
| **Discord 시작 알림** | 팀에 배포 시작 공지 | 알림 없이 진행 |
| **CI 아티팩트 다운로드** | 빌드 결과물 가져오기 | 배포 중단 |
| **SCP 전송** | 아티팩트를 EC2로 전송 | 배포 중단 |
| **SSH 접속** | EC2 서버 연결 | 배포 중단 |
| **서비스 중단** | 기존 프로세스 정지 | (BE/AI만 해당) |
| **아티팩트 배포** | 파일 교체/복사 | 배포 중단, 롤백 필요 |
| **서비스 시작** | 프로세스 재시작 | (BE/AI만 해당) |
| **헬스체크** | 서비스 정상 동작 확인 | **롤백 실행** |
| **Discord 완료 알림** | 팀에 배포 결과 공지 | - |

<br>

### 5.2 CI 아티팩트 활용

#### 5.2.1 **서버 빌드 vs 아티팩트 배포 비교 :아티팩트 배포 선택 이유**

| 방식 | 서버 직접 빌드 | 아티팩트 배포 |
|------|--------------|-------------|
| 빌드 환경 | 서버 (운영 중) | CI Runner (격리됨) |
| 다운타임 | 빌드 시간 포함 (3-5분) | 파일 교체만 (수십 초) |
| 서버 부하 | 빌드 중 CPU/메모리 사용 | 없음 |
| 일관성 | 서버 상태에 의존 | CI 환경에서 검증 완료 |
| 롤백 | 이전 커밋 체크아웃 후 재빌드 | 이전 아티팩트 다운로드 |


1. **다운타임 최소화**
   - 서버 빌드 방식은 빌드 시간(BE 기준 3분+) 동안 서비스 중단 발생
   - 아티팩트 배포는 파일 교체만 수행하므로 다운타임이 수십 초로 단축

2. **서버 리소스 보호**
   - m7g.large 인스턴스(2vCPU, 8GB RAM)에서 빌드 시 CPU/메모리 점유 발생
   - CI Runner(격리된 환경)에서 빌드하면 운영 서버에 영향 없음

3. **빌드 환경 일관성**
   - CI에서 동일한 Ubuntu 환경으로 빌드하여 "로컬에서는 되는데" 문제 방지
   - CI 통과 = 배포 가능한 상태 보장

4. **롤백 용이성**
   - 아티팩트가 GitHub에 7일간 보관되어 이전 버전으로 빠른 복구 가능

#### 5.2.2 아티팩트 활용 흐름

```
[CI 완료]
    │
    ▼
GitHub Artifacts 저장소
    │ (빌드 결과물 7일 보관)
    ▼
[CD 시작 - workflow_dispatch]
    │
    ▼
actions/download-artifact
    │ (아티팩트 다운로드)
    ▼
SCP/rsync로 EC2 전송
    │
    ▼
서비스 재시작
    │
    ▼
헬스체크 ──실패──▶ 롤백 절차 돌입
    │
    ▼ (성공)
Discord 성공 알림
```

### 5.3 서버별 특이사항 요약

| 서버 | 아티팩트 | 배포 방식 | 롤백 방식 | 예상 다운타임 |
|------|---------|----------|----------|-------------|
| **Frontend** | `build/` 폴더 | Nginx 디렉토리 교체 | 백업 폴더 복원 | 0분 |
| **Backend** | JAR 파일 | systemd 서비스 재시작 | 백업 JAR 복원 | ~30초 |
| **AI Server** | 없음 (git pull) | systemd 서비스 재시작 | git checkout | ~30초 |

<br>

## 6. 실패 시 대응 방안

### 6.1 롤백 절차

**아티팩트 기반 롤백:**

```bash
# 1. 이전 CI 실행에서 아티팩트 다운로드
# GitHub Actions UI → 이전 성공한 CI run → Artifacts → 다운로드

# 2. 수동으로 서버에 업로드 및 교체
scp -i key.pem artifact.zip ubuntu@server:/tmp/
ssh -i key.pem ubuntu@server "unzip /tmp/artifact.zip -d /app/"

# 3. 서비스 재시작
ssh -i key.pem ubuntu@server "sudo systemctl restart service-name"
```

**Git 기반 롤백 (아티팩트 없을 경우):**

```bash
# 1. 이전 커밋으로 체크아웃
git checkout <previous-commit-hash>

# 2. 재빌드 (다운타임 발생)
./gradlew build -x test  # Backend
npm run build            # Frontend

# 3. 서비스 재시작
```

### 6.2 재시도 방식

| 방식 | 사용 상황 | 방법 |
|------|----------|------|
| **수동 Re-run** | 일시적 네트워크 오류 | GitHub Actions UI에서 "Re-run jobs" |
| **새 워크플로우 실행** | 설정 수정 후 재시도 | workflow_dispatch 다시 실행 |

<br>

## 현재 작업 상황 (2026-01-26 기준)

> 설계 이후 구현 과정에서 추가/변경된 사항입니다. 상세 내용은 각 서버별 CD 설정 가이드를 참조하세요.

### AI Server

| 항목 | 설계 | 현재 구현 | 비고 |
|------|------|----------|------|
| CD 트리거 | workflow_dispatch (수동) | workflow_run (CI 완료 후 자동) | 자동화 |
| 인증 방식 | SSH | GCP OIDC (Workload Identity) | 보안 강화 |
| 배포 방식 | git pull | 아티팩트 배포 (tar.gz) | 일관성 확보 |
| 방화벽 | 고정 규칙 | 동적 생성/삭제 | 보안 강화 |
| 패키지 설치 | 매번 설치 | uv.lock 해시 비교 후 조건부 | 속도 개선 |
| 롤백 방식 | 수동 (아티팩트 다운로드) | CD Re-run (해당 시점 아티팩트 재배포) | BE/FE와 통일 |

<br>

## 7. 다음 단계

이 문서는 CD 개요를 다룬다. 각 서버별 구체적인 CD 설정은 아래 문서를 참고:

- Frontend: [[Frontend CD 설정 가이드]]
- Backend: [[Backend CD 설정 가이드]]
- AI Server: [[AI Server CD 설정 가이드]]