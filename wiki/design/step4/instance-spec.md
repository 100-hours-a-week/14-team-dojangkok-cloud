# V2 AWS 인프라 구성 인스턴스 사양 선정

## 1. 선정 사양 요약
| 인스턴스 | 사양 |
| :---- | :---- |
| NAT Instance | t4g.nano |
| 프론트엔드(Next.js) | t4g.small |
| 백엔드(Spring Boot) | t4g.small |
| 토큰 스토리지, 인메모리DB(Redis) | t4g.small |
| 메시지큐(RabbitMQ) | t4g.small |
| 모니터링 서버(PLG) | t4g.medium |
| 관계형DB(MySQL) | t4g.medium |

## 2. 선정 원칙

도장콕은 MAU 5만을 목표로 하는 초기 스타트업 서비스로, 한정된 예산 내에서 서비스 안정성과 비용 효율성을 동시에 확보해야 합니다. 인스턴스 사양 선정 시 다음 원칙을 적용하였습니다.

1. **실사용량 기반 적정 산정**: 각 서비스의 메모리·CPU 사용 특성을 분석하여 과잉 할당 없이 안정 운영에 필요한 최소 사양을 산정
2. **비용 효율성 우선**: 월 인프라 비용을 최소화하되, 서비스 안정성을 저해하지 않는 선에서 결정
3. **수평 확장 전략과의 정합성**: ASG 기반 수평 확장을 전제로, 단일 인스턴스를 과도하게 키우기보다 적정 사양의 인스턴스를 수량으로 대응
4. **버스터블 인스턴스 활용**: 도장콕의 핵심 사용 시간대는 부동산 영업시간인 09~21시에 집중되며, 야간·새벽 시간대는 트래픽이 매우 낮습니다. 이 패턴은 DB(MySQL, Redis)·메시지큐(RabbitMQ) 등 Stateful 서비스를 포함한 모든 인프라 서비스에 동일하게 적용됩니다. DB와 메시지큐는 자체적으로 트래픽을 생성하지 않고 애플리케이션 트래픽에 비례하여 부하를 받기 때문입니다. 따라서 야간 약 12시간의 유휴 시간에 CPU 크레딧을 충분히 축적하고, 피크 시간대에 버스트할 수 있는 T4g 패밀리를 전체 인스턴스에 적용하였습니다. 운영 단계에서는 CloudWatch를 통해 CPU 크레딧 잔량(`CPUCreditBalance`)을 지속적으로 모니터링하며, 크레딧이 만성적으로 부족해지는 인스턴스가 식별되면 해당 인스턴스를 범용 인스턴스(m7g 등)로 전환할 계획입니다.

### 참고: t4g 인스턴스 패밀리 사양

> 월 비용은 서울 리전(ap-northeast-2) On-Demand 기준 근사값 (730시간/월)

| 인스턴스 타입 | vCPU | 메모리 | 네트워크 | 시간당 비용 | 월 비용 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| t4g.nano | 2 | 0.5 GiB | Up to 5 Gbps | $0.0042 | ~$3.07 |
| t4g.small | 2 | 2 GiB | Up to 5 Gbps | $0.0153 | ~$11.17 |
| t4g.medium | 2 | 4 GiB | Up to 5 Gbps | ~$0.0306 | ~$22.34 |


## 3. 인스턴스별 사양 선정 근거

### 1. NAT Instance — t4g.nano (2 vCPU / 0.5 GiB)

#### 역할
Private Subnet에 위치한 인스턴스들의 아웃바운드 인터넷 접근을 중계하는 NAT 기능과 운영자의 SSH 접근을 위한 Bastion Host 역할을 겸합니다.

#### 선정 근거

1. **낮은 트래픽 부하**: 도장콕의 아웃바운드 트래픽은 OAuth 2.0 인증 요청, 외부 API 호출, 패키지 업데이트 등으로 구성되며 그 빈도가 비교적 낮습니다. NAT 기능은 커널 수준의 패킷 포워딩(iptables)으로 처리되므로 CPU·메모리 부하가 미미합니다.
2. **메모리 요구량 최소**: Linux OS 운영 + iptables NAT 테이블 유지에 0.5 GiB면 충분합니다. NAT 인스턴스는 별도 애플리케이션을 실행하지 않으므로 추가 메모리가 불필요합니다.
3. **비용 절감 효과**: AWS NAT Gateway(~$32/월 + 데이터 처리 비용) 대비 t4g.nano(~$3/월)로 월 약 $29 이상 절감이 가능합니다. 도장콕의 현재 아웃바운드 트래픽 규모에서는 NAT Gateway가 제공하는 고가용성·고성능이 불필요하다고 판단했습니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| — (하위 없음) | t4g.nano가 t4g 패밀리의 최소 사양 |
| t4g.micro (1 GiB) | 현재 워크로드 대비 메모리 과잉, 비용 증가에 비해 실익 없음 |

> **확장 시점**: 아웃바운드 트래픽이 지속 증가하여 네트워크 대역폭이 병목이 되는 경우 t4g.micro로의 수직 확장 또는 NAT Gateway 전환을 검토합니다.

### 2. 프론트엔드(Next.js) — t4g.small (2 vCPU / 2 GiB)

#### 역할
Next.js 기반 SSR(Server-Side Rendering)을 수행하고 정적 자산을 서빙합니다. ALB를 통해 사용자 요청의 최초 진입점 역할을 합니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| Node.js 런타임 기본 | ~200–300 MB |
| SSR 렌더링 버퍼 (피크 시) | ~300–500 MB |
| **합계** | **~800 MB–1.2 GB** |

#### 선정 근거

1. **SSR 메모리 요구**: Node.js 프로세스의 기본 메모리 사용량(~200–300MB)에 SSR 렌더링 시 페이지별 데이터 페칭·렌더링 버퍼를 합산하면 약 800MB–1.2GB가 필요합니다. OS 및 Docker 오버헤드를 포함하면 2 GiB가 적정 수준입니다.
2. **수평 확장 시 비용 효율성**: 동일 예산으로 t4g.medium 대비 더 많은 인스턴스를 확보할 수 있어, ASG 기반 수평 확장에서 비용 효율이 높습니다.

    | 구성 (동일 예산 ~$44.68/월) | 인스턴스 수 | 총 vCPU | 총 메모리 |
    | :---- | :---- | :---- | :---- |
    | t4g.medium × 2 | 2대 | 4 vCPU | 8 GiB |
    | **t4g.small × 4** | **4대** | **8 vCPU** | **8 GiB** |

    같은 비용에서 총 vCPU가 2배이며, 인스턴스 수가 많아 1대 장애 시 용량 손실이 적고(50% → 25%), 더 작은 단위로 스케일링하여 비용 낭비를 줄일 수 있습니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.micro (1 GiB) | OS + Docker 오버헤드를 제외하면 Node.js에 할당 가능한 메모리가 ~600MB에 불과하여, SSR 렌더링 피크 시 OOM 위험 |
| t4g.medium (4 GiB) | 프론트엔드 워크로드 대비 메모리 과잉이며, 동일 예산 대비 확보 가능한 인스턴스 수와 총 vCPU가 절반으로 줄어 수평 확장 효율이 저하됨 |

### 3. 백엔드(Spring Boot) — t4g.small (2 vCPU / 2 GiB)

#### 역할
REST API 처리, 비즈니스 로직 수행, SSE 연결 관리, RabbitMQ를 통한 GCP AI 서버 연동 등 서비스의 핵심 로직을 담당합니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| JVM Heap (`-Xmx512m`) | 512 MB |
| JVM Metaspace | ~100–150 MB |
| 스레드 스택 (~30 스레드) | ~30–50 MB |
| NIO 버퍼 / 기타 | ~50–100 MB |
| SSE 연결 유지 (SseEmitter 등) | ~50–100 MB |
| **합계** | **~1.0–1.3 GB** |

#### 선정 근거

1. **JVM 메모리 설계**: Heap을 512MB로 제한(`-Xms512m -Xmx512m`)하고, Metaspace·스레드 스택·SSE 연결 오버헤드 등을 포함한 JVM 총 RSS를 ~800–900MB 내외로 관리합니다. OS·Docker 오버헤드를 합산하면 2 GiB 내에서 운영이 가능합니다.
2. **수평 확장 시 비용 효율성**: 프론트엔드와 동일하게, 동일 예산으로 t4g.medium 대비 더 많은 인스턴스를 확보할 수 있어 ASG 기반 수평 확장에서 비용 효율이 높습니다.

    | 구성 (동일 예산 ~$44.68/월) | 인스턴스 수 | 총 vCPU | 총 메모리 |
    | :---- | :---- | :---- | :---- |
    | t4g.medium × 2 | 2대 | 4 vCPU | 8 GiB |
    | **t4g.small × 4** | **4대** | **8 vCPU** | **8 GiB** |

    같은 비용에서 총 vCPU가 2배이며, 인스턴스 수가 많아 1대 장애 시 용량 손실이 적고(50% → 25%), 더 작은 단위로 스케일링하여 비용 낭비를 줄일 수 있습니다.
3. **SSE 연결 분산**: 도장콕의 백엔드는 AI 추론 결과를 실시간으로 전달하기 위해 SSE 연결을 유지합니다. 인스턴스 수가 많을수록 ALB를 통해 SSE 연결이 더 많은 인스턴스로 분산되므로, 개별 인스턴스의 메모리 부담을 낮출 수 있습니다.
4. **SWAP 메모리를 통한 OOM 안전망 확보**: 일시적인 트래픽 스파이크로 물리 메모리 사용량이 급증하는 경우에 대비하여 SWAP 메모리를 설정합니다. 이를 통해 OOM Kill로 인한 프로세스 종료를 방지하고, ASG가 스케일아웃을 완료하기까지의 시간을 확보합니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.micro (1 GiB) | JVM 프로세스만으로도 메모리가 부족하여 정상 기동이 불가 |
| t4g.medium (4 GiB) | Heap을 더 크게 잡을 수 있으나 MAU 5만 규모에서는 Heap 512MB로 충분하며, 동일 예산 대비 확보 가능한 인스턴스 수와 총 vCPU가 절반으로 줄어 수평 확장 효율이 저하됨 |


### 4. 토큰 스토리지, 인메모리DB(Redis) — t4g.small (2 vCPU / 2 GiB)

#### 역할
사용자의 JWT Refresh Token 저장 및 토큰 블랙리스트 관리를 주 용도로 사용하는 인메모리 데이터 스토어입니다. 현재 단계에서는 애플리케이션 데이터 캐싱은 거의 사용하지 않으며, 향후 서비스 성장에 따라 캐싱 용도를 점진적으로 확대할 계획입니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| Redis 프로세스 기본 | ~50–100 MB |
| Refresh Token 데이터 (MAU 5만 기준) | ~25–50 MB |
| 향후 캐싱 확장 여유분 | ~200–500 MB |
| **합계** | **~575 MB–1.05 GB** |

> Refresh Token 산정: MAU 5만 × 토큰당 ~500 bytes = ~25MB. 동시 접속 피크 기준이 아닌 전체 활성 사용자 기준으로 산정하여 여유를 확보했습니다.

#### 선정 근거

1. **Redis의 경량성**: Redis는 싱글 스레드 기반으로 동작하며 기본 프로세스 메모리가 ~50–100MB로 매우 경량입니다. CPU 사용률도 낮아 버스터블 인스턴스에 적합합니다.
2. **현재 워크로드 대비 여유 확보**: 현재 주 용도인 Refresh Token 저장만으로는 메모리 사용량이 매우 낮지만(~25–50MB), 향후 캐싱 도입 시 별도의 인프라 변경 없이 수용할 수 있도록 여유를 확보했습니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.micro (1 GiB) | 현재 토큰 저장만으로는 충분하나, 향후 캐싱 도입 시 가용 메모리(~600MB)가 부족할 수 있어 인스턴스 교체가 필요해짐 |
| t4g.medium (4 GiB) | 현재 및 단기 계획 대비 메모리 과잉, 비용 대비 실익 없음 |


### 5. 메시지큐(RabbitMQ) — t4g.small (2 vCPU / 2 GiB)

#### 역할
AWS(Spring Boot)와 GCP(FastAPI) 간 비동기 통신을 중계하는 메시지 브로커입니다. RPC 패턴(Reply-To + Correlation-ID)을 통해 AI 추론 요청/응답을 처리합니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| Erlang VM + RabbitMQ 프로세스 | ~300–500 MB |
| 메시지 버퍼 / 큐 데이터 | ~100–300 MB |
| **합계** | **~700 MB–1.2 GB** |

#### 선정 근거

1. **Erlang VM 오버헤드**: RabbitMQ는 Erlang VM 위에서 동작하며, 런타임 포함 기본 메모리가 ~300–500MB로 Node.js나 Redis보다 높습니다.
2. **제한된 큐 깊이**: 도장콕에서 RabbitMQ를 통과하는 주요 메시지는 AI 추론 요청입니다. GPU 인스턴스의 처리 속도에 의해 동시 큐 깊이가 자연스럽게 제한되므로, 대규모 메시지 적재로 인한 메모리 급증 가능성은 낮습니다.
3. **메시지 영속성**: 메시지 유실 방지를 위해 durable queue를 사용하여 디스크 I/O가 발생하나, 개별 메시지 크기(AI 요청 파라미터)가 작아 t4g.small의 I/O 성능으로 충분합니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.micro (1 GiB) | Erlang VM + RabbitMQ 프로세스에 OS·Docker 오버헤드를 합산하면 1 GiB로는 안정적 운영이 어려움 |
| t4g.medium (4 GiB) | 예상 큐 깊이와 메시지 규모 대비 메모리 과잉 |


### 6. 모니터링 서버(PLG) — t4g.medium (2 vCPU / 4 GiB)

#### 역할
Prometheus(메트릭 수집·저장), Loki(로그 수집·저장), Grafana(시각화·대시보드)를 단일 인스턴스에서 운영하여 AWS·GCP 전체 인프라를 통합 모니터링합니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| Prometheus (TSDB + 스크래핑) | ~1.0–1.5 GB |
| Loki (인덱스 + 청크 캐시) | ~500 MB–1.0 GB |
| Grafana (대시보드 렌더링) | ~200–300 MB |
| **합계** | **~2.0–3.2 GB** |

#### 선정 근거

1. **다중 서비스 동시 운영**: 3개의 독립 서비스(Prometheus, Loki, Grafana)를 하나의 인스턴스에서 컨테이너로 동시 운영하므로, 각 서비스의 메모리 사용량이 합산됩니다.
2. **Prometheus 메모리 특성**: Prometheus는 스크래핑 대상(target)이 증가할수록 시계열 데이터의 인메모리 사용량이 비례하여 증가합니다. 현재 AWS 인스턴스 7대 + GCP 인스턴스를 대상으로 메트릭을 수집하므로 최소 1GB 이상이 필요합니다.
3. **모니터링 안정성 확보**: 모니터링은 장애 탐지·대응의 핵심이므로, 모니터링 서버 자체가 메모리 부족으로 인한 OOM Kill로 다운되어 모니터링 공백이 발생하는 상황을 반드시 방지해야 합니다. 이를 위해 충분한 메모리 여유를 확보했습니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.small (2 GiB) | 3개 서비스의 합산 메모리 요구량(~2.0–3.2GB)을 수용할 수 없어 동시 운영이 물리적으로 불가 |
| t4g.large (8 GiB) | 현재 모니터링 대상 규모(인스턴스 ~10대)에서는 과잉이며, 데이터 보존 기간(retention) 조정으로 메모리 사용량 통제 가능 |

> **확장 시점**: ASG로 인스턴스 수가 증가하면 Prometheus의 스크래핑 대상과 시계열 데이터도 비례하여 증가합니다. 메모리 사용량이 지속적으로 상한에 근접하는 경우 t4g.large(8 GiB)로의 수직 확장을 검토합니다.


### 7. 관계형DB(MySQL) — t4g.medium (2 vCPU / 4 GiB)

#### 역할
사용자 정보, 매물 데이터, 계약 요약 정보, 집노트 등 서비스의 핵심 영속 데이터를 저장·관리하는 주 데이터베이스입니다.

#### 메모리 산정

| 구성 요소 | 예상 메모리 사용량 |
| :---- | :---- |
| Linux OS + Docker 데몬 | ~300–400 MB |
| InnoDB Buffer Pool | ~1.5 GB |
| 커넥션당 메모리 (× 30 커넥션) | ~150–300 MB |
| 임시 테이블 / 정렬 버퍼 | ~100–200 MB |
| **합계** | **~2.0–2.4 GB** |

#### 선정 근거

1. **InnoDB Buffer Pool**: MySQL 성능의 핵심인 Buffer Pool을 ~1.5GB로 설정합니다. 이는 총 메모리의 약 50~70% 수준으로 MySQL 권장 범위에 해당하며, 최근 7일간의 활성 데이터와 인덱스를 메모리에 캐시하여 디스크 I/O를 최소화합니다.
2. **부하테스트 근거**: 기존 실측에서 CCU 200 혼합 시나리오(읽기 80%)에서 안정적으로 동작함을 확인했습니다(p95 80ms, 에러율 4.4%). 이는 MAU 5만의 피크 동시접속 규모를 충분히 수용하는 수준입니다.
3. **데이터 안전성**: 도장콕의 핵심 자산인 사용자 계약 정보를 다루는 DB이므로, 메모리 부족으로 인한 예기치 않은 프로세스 종료(OOM Kill) 위험을 배제하기 위해 적정 여유를 확보했습니다.

#### 하위·상위 사양 배제

| 대안 | 배제 사유 |
| :---- | :---- |
| t4g.small (2 GiB) | Buffer Pool을 500MB 이하로 제한해야 하며, 캐시 히트율 급감으로 디스크 I/O 병목 발생, 쿼리 응답 시간 크게 증가 |
| t4g.large (8 GiB) | 현재 데이터 규모(MAU 5만) 대비 과잉이며, 수직 확장은 트래픽 증가가 실측될 때 진행하는 것이 비용 효율적 |

> **확장 시점**: CCU 300 이상 또는 쓰기 집중 워크로드 증가 시 t4g.large(8 GiB)로의 수직 확장을 검토하고, 이후 Read Replica 추가를 통한 읽기 분산을 고려합니다.


## 4. 월간 인스턴스 비용 요약

> 최소 구성(ASG min=1) 기준, 서울 리전 On-Demand 가격

| 인스턴스 | 사양 | 수량 | 월 비용 |
| :---- | :---- | :---- | :---- |
| NAT Instance | t4g.nano | 1 | ~$3.07 |
| 프론트엔드(Next.js) | t4g.small | 1 | ~$11.17 |
| 백엔드(Spring Boot) | t4g.small | 1 | ~$11.17 |
| Redis | t4g.small | 1 | ~$11.17 |
| RabbitMQ | t4g.small | 1 | ~$11.17 |
| 모니터링 서버(PLG) | t4g.medium | 1 | ~$22.34 |
| MySQL | t4g.medium | 1 | ~$22.34 |
| **합계** | | **7대** | **~$92.43** |

> 위 금액은 EC2 인스턴스 비용만 포함하며, EBS 스토리지·데이터 전송·ALB 등 기타 비용은 별도입니다.
